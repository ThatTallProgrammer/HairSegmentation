{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.backend import set_session, clear_session, get_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted categorical crossentropy\n",
    "#\n",
    "# source: https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d\n",
    "#\n",
    "\n",
    "def weighted_cce(y_true, y_pred, weights=np.array([4,1,1])):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    # scale predictions so that the class probas of each sample sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    # clip to prevent NaN's and Inf's\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calc\n",
    "    loss = y_true * K.log(y_pred) * weights\n",
    "    loss = -K.sum(loss, -1)\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the dice coefficient of a \n",
    "# predicted and true image tensors\n",
    "#\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    # calculate the magnitude of the intersection between\n",
    "    # the predicted and true tensors\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    \n",
    "    # calculate the magnitude of the union of the \n",
    "    # predicted and true tensors\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    \n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_np_array_images_from_directory(dir_path, image_files=None):\n",
    "    \n",
    "    # container for image arrays\n",
    "    np_arrays = []\n",
    "    \n",
    "    # if image_files wasn't specified read all images from directory\n",
    "    if image_files is None:\n",
    "        image_files = os.listdir(dir_path)\n",
    "        image_files = sorted(image_files, key=lambda s:int(s.split('_')[2].split('.')[0]))\n",
    "    \n",
    "    # construct the appropriate absolute paths for each image\n",
    "    image_paths = [os.path.join(dir_path, filename) for filename in image_files]\n",
    "    \n",
    "    # read, resize, and convert each image to a Numpy array\n",
    "    for path in image_paths:\n",
    "        \n",
    "        # open image\n",
    "        img = Image.open(path)\n",
    "        img = img.convert(\"RGB\")\n",
    "        \n",
    "        # resize image\n",
    "        img = img.resize((256, 256))\n",
    "        \n",
    "#         # convert to binary image if is_mask is True\n",
    "#         if is_mask:\n",
    "#             img = img.convert(\"1\")\n",
    "        \n",
    "        # convert image to Numpy array \n",
    "        array = np.array(img)\n",
    "        \n",
    "#         # ensure mask images have a single color channel dimension\n",
    "#         if is_mask:\n",
    "#             array = array.reshape(256, 256, 1)\n",
    "        \n",
    "        # add the image to the image arrays container\n",
    "        np_arrays.append(array)\n",
    "        \n",
    "    return np.array(np_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all imagese in the predictions \n",
    "# directory\n",
    "#\n",
    "\n",
    "def clear_old_predictions(predictions_dir):\n",
    "    files = os.listdir(predictions_dir)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(predictions_dir, file)\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a Numpy array containing the information \n",
    "# for a mask image to a pillow image\n",
    "#\n",
    "\n",
    "def get_mask_from_binary_array(np_array) -> Image:\n",
    "    \n",
    "    # extract dimensions of image\n",
    "    height = np_array.shape[0]\n",
    "    width = np_array.shape[1]\n",
    "    \n",
    "    # cut off the channels dimension\n",
    "    # which should be 1\n",
    "    np_array = np_array.reshape(height, width)\n",
    "    \n",
    "    # convert to 255 black and white representation\n",
    "    np_array *= 255\n",
    "    \n",
    "    return Image.fromarray(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu configuration for this system\n",
    "#\n",
    "\n",
    "config = tf.compat.v1.ConfigProto( device_count={'GPU': 1, 'CPU':8})\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model_dir = \"top_models\"\n",
    "\n",
    "model_filenames = os.listdir(top_model_dir)\n",
    "\n",
    "model_filepaths = [os.path.join(top_model_dir, filename) for filename in model_filenames]\n",
    "\n",
    "dependencies = {\n",
    "    \"dice_coef\": dice_coef,\n",
    "    \"dice_loss\": dice_loss,\n",
    "    \"weighted_cce\": weighted_cce\n",
    "}\n",
    "\n",
    "testing_images_dir = \"celeb_pics/\"\n",
    "predictions_dir = \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_image_arrays = get_np_array_images_from_directory(testing_images_dir)\n",
    "testing_image_arrays = testing_image_arrays.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top_models\\\\fcn8_1.hdf5', 'top_models\\\\fcn8_2_789.hdf5', 'top_models\\\\fcn8_4_798.hdf5', 'top_models\\\\unet_1_790.hdf5', 'top_models\\\\unet_2_807.hdf5']\n"
     ]
    }
   ],
   "source": [
    "print(model_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_filepaths[4], custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testing_image_arrays, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filepath in model_filepaths[1:]:\n",
    "#     model = load_model(filepath, custom_objects=dependencies)\n",
    "#     predictions += model.predict(testing_image_arrays, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (predictions.argmax(axis=-1) == 0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the predictions directory\n",
    "#\n",
    "clear_old_predictions(predictions_dir)\n",
    "\n",
    "save_tag = \"test_mask\"\n",
    "\n",
    "i = 1\n",
    "for c in classes:\n",
    "    img = get_mask_from_binary_array(c)\n",
    "    img = img.resize((250, 250))\n",
    "    img.save(os.path.join(predictions_dir, \"{}_{}.png\".format(save_tag, str(i))), format=\"PNG\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
